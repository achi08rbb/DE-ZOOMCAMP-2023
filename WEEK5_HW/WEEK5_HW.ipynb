{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "choice-lease",
   "metadata": {},
   "source": [
    "# Import Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fundamental-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "divided-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=(SparkSession.builder\n",
    "       .master('local[*]')\n",
    "       .appName('homework')\n",
    "       .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-context",
   "metadata": {},
   "source": [
    "## Question 1. Checking spark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entertaining-theory",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-integrity",
   "metadata": {},
   "source": [
    "## Question 2. Reading csv to spark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "plastic-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv=spark.read.option('header','true').csv('fhvhv_tripdata_2021-06.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lyric-hundred",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropoff_datetime', StringType(), True), StructField('PULocationID', StringType(), True), StructField('DOLocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check schema\n",
    "df_fhv.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chinese-david",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02764|2021-06-01 00:02:41|2021-06-01 00:07:46|         174|          18|      N|                B02764|\n",
      "|              B02764|2021-06-01 00:16:16|2021-06-01 00:21:14|          32|         254|      N|                B02764|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "micro-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import types\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "detected-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed schema\n",
    "fhv_schema=types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True), \n",
    "    types.StructField('PULocationID', types.IntegerType(), True), \n",
    "    types.StructField('DOLocationID', types.IntegerType(), True), \n",
    "    types.StructField('SR_Flag', types.StringType(), True), \n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "martial-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df=(spark.read\n",
    "                .option('header','true')\n",
    "                .schema(fhv_schema)\n",
    "                .csv('fhvhv_tripdata_2021-06.csv.gz')\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "backed-credits",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/rbbel/notebooks/pq_files/fhvhv/2021/06 already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3f62094b52ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransformed_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pq_files/fhvhv/2021/06/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     def text(\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/rbbel/notebooks/pq_files/fhvhv/2021/06 already exists."
     ]
    }
   ],
   "source": [
    "transformed_df.repartition(12).write.parquet('pq_files/fhvhv/2021/06/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-indonesian",
   "metadata": {},
   "source": [
    "## Question 3. Count Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "protective-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_pq=spark.read.parquet('pq_files/fhvhv/2021/06/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "closing-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fhv_pq.createOrReplaceTempView('fhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "greenhouse-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=spark.sql(\"\"\"\n",
    "SELECT COUNT(*) as trip_number\n",
    "\n",
    "FROM fhv\n",
    "\n",
    "WHERE pickup_datetime LIKE \"2021-06-15%\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "looking-radical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|trip_number|\n",
      "+-----------+\n",
      "|     452470|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-chester",
   "metadata": {},
   "source": [
    "### Question 4. Longest Trip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "grave-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function computing the pickup - dropoff\n",
    "import pyspark.sql.functions as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "harmful-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New df with added trip_duration column\n",
    "trips=df_fhv_pq \\\n",
    "        .withColumn('trip_duration', (df_fhv_pq['dropoff_datetime']) - (df_fhv_pq['pickup_datetime']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "former-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df with only the trip_duration column\n",
    "duration=trips \\\n",
    "        .select('trip_duration') \\\n",
    "        .orderBy(trips['trip_duration'].desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "partial-tyler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[trip_duration: interval day to second]>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check trip_duration column type\n",
    "duration.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "blank-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df with converted trip_duration column () from interval day to second to interval hour to minute\n",
    "duration_hrs=duration \\\n",
    "     .withColumn('duration_hrs', ((duration['trip_duration'].cast('INTERVAL MINUTE')).cast('INTEGER')/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "appreciated-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|       trip_duration|      duration_hrs|\n",
      "+--------------------+------------------+\n",
      "|INTERVAL '2 18:52...| 66.86666666666666|\n",
      "|INTERVAL '1 01:32...|25.533333333333335|\n",
      "|INTERVAL '0 19:58...|19.966666666666665|\n",
      "|INTERVAL '0 18:11...|18.183333333333334|\n",
      "|INTERVAL '0 16:28...|16.466666666666665|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "duration_hrs.orderBy(duration_hrs['duration_hrs'].desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-retention",
   "metadata": {},
   "source": [
    "## Question 6 Most frequent pickup location zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "certified-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone=spark.read.option('header','true').csv('taxi_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "subjective-amino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_zone.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "primary-canon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B02764|2021-06-30 20:30:27|2021-06-30 20:46:23|         247|         168|      N|                B02764|\n",
      "|              B02876|2021-06-08 14:42:05|2021-06-08 15:05:06|          76|          17|      N|                B02876|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fhv_pq.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "empty-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "representative-mississippi",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zones' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-e43af58ff453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzones\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'zones' is not defined"
     ]
    }
   ],
   "source": [
    "zones.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "clear-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=spark.sql(\"\"\"\n",
    "SELECT z1.Zone, COUNT(*) as Frequency\n",
    "\n",
    "FROM fhv\n",
    "INNER JOIN zones AS z1 on z1.LocationID=fhv.DOLocationID\n",
    "INNER JOIN zones AS z2 on z2.LocationID=fhv.DOLocationID\n",
    "\n",
    "WHERE z1.Zone != 'NA'\n",
    "GROUP BY z1.Zone\n",
    "ORDER BY Frequency DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "drawn-panic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|               Zone|Frequency|\n",
      "+-------------------+---------+\n",
      "|Crown Heights North|   236244|\n",
      "|        JFK Airport|   224571|\n",
      "|     Bushwick South|   187946|\n",
      "|      East New York|   186038|\n",
      "|  LaGuardia Airport|   182694|\n",
      "+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-hormone",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
